{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52686129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MariaDB successful!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "# Third-party library imports\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import sqlalchemy as sa\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from geopy.distance import geodesic\n",
    "import polyline  # Install: pip install polyline\n",
    "from shapely import wkt\n",
    "from shapely.geometry import LineString, Point\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "import polyline\n",
    "# Local application/library imports\n",
    "from db_tables import *\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from shapely.geometry import Point\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely.strtree import STRtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454e6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIGURATION ----------\n",
    "API_KEY = 'AIzaSyCawuGvoiyrHOh3RyJdq7yzFCcG5smrZCI'  # Replace with your actual API key\n",
    "CITY_NAME = \"Košice, Slovakia\"#\"New York City, New York, USA\"#\"Košice, Slovakia\"\n",
    "N_VEHICLES = 10\n",
    "K_ALTERNATIVES = 3  # Number of route alternatives per vehicle\n",
    "MIN_LENGTH = 0\n",
    "MAX_LENGTH = 10000\n",
    "TIME_STEP = 10\n",
    "TIME_WINDOW = 600\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af12866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_at_percentage(line_wkt, percentage):\n",
    "    # Parse the WKT LINESTRING to a LineString object\n",
    "    line = LineString([(21.2159377, 48.7126189), (21.2159939, 48.7125398), (21.2162822, 48.7121463)])  # Example coordinates\n",
    "\n",
    "    # Calculate the total length of the line\n",
    "    total_length = line.length\n",
    "\n",
    "    # Calculate the target distance based on the percentage of the total length\n",
    "    target_length = total_length * percentage\n",
    "\n",
    "    # Now, we need to interpolate the point at target_length\n",
    "    # Use the interpolate function of shapely's LineString\n",
    "    point_at_percentage = line.interpolate(target_length)\n",
    "\n",
    "    return point_at_percentage.x, point_at_percentage.y\n",
    "\n",
    "def create_geodataframe_from_coords(coords):\n",
    "    # Create a list of Point geometries from the coordinates\n",
    "    points = [Point(coord['lng'], coord['lat']) for coord in coords]\n",
    "    \n",
    "    # Create a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(geometry=points)\n",
    "    \n",
    "    # Set the coordinate reference system (CRS) to WGS84 (EPSG:4326)\n",
    "    gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "\n",
    "def create_linestring_from_polyline(polyline_points):\n",
    "    # Decode the polyline string into a list of coordinates (lat, lng)\n",
    "    #decoded_points = polyline.decode(polyline_points, precision=6)  # Adjust precision as needed\n",
    "    # Create a LineString from the decoded points\n",
    "    line = LineString(polyline_points)\n",
    "    \n",
    "    # Create a GeoDataFrame to store the LineString\n",
    "    gdf = gpd.GeoDataFrame(geometry=[line])\n",
    "    \n",
    "    # Set CRS to WGS84 (EPSG:4326)\n",
    "    gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "\n",
    "\n",
    "def get_point_on_line(line, percentage):\n",
    "    \"\"\"\n",
    "    Get a point on the line at a certain percentage of the total length of the line.\n",
    "    \n",
    "    :param line: A Shapely LineString geometry.\n",
    "    :param percentage: A float between 0 and 1 indicating the percentage along the line.\n",
    "    :return: A Shapely Point geometry at the specified percentage of the line.\n",
    "    \"\"\"\n",
    "    # Ensure the percentage is between 0 and 1\n",
    "    if not 0 <= percentage <= 1:\n",
    "        raise ValueError(\"Percentage must be between 0 and 1.\")\n",
    "    \n",
    "    # Calculate the total length of the line\n",
    "    total_length = line.length\n",
    "    \n",
    "    # Calculate the target distance along the line based on the percentage\n",
    "    target_distance = total_length * percentage\n",
    "    \n",
    "    # Get the point at the target distance along the line\n",
    "    point_on_line = line.interpolate(target_distance)\n",
    "    \n",
    "    return point_on_line\n",
    "\n",
    "def get_routes_from_google(origin, destination, api_key,max_nr_of_alternative_routes):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
    "    params = {\n",
    "        \"origin\": f\"{origin[0]},{origin[1]}\",  # Latitude, Longitude of origin\n",
    "        \"destination\": f\"{destination[0]},{destination[1]}\",  # Latitude, Longitude of destination\n",
    "        \"mode\": \"driving\",  # Mode of transport\n",
    "        \"alternatives\": \"true\",  # Request alternative routes\n",
    "        \"departure_time\": \"now\",  # Immediate departure\n",
    "        \"key\": api_key  # Your API key\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        routes = response.json().get(\"routes\", [])\n",
    "        return routes[:max_nr_of_alternative_routes]  # Return only the first 3 routes if available\n",
    "    else:\n",
    "        print(response.text)\n",
    "    return None\n",
    "\n",
    "def calculate_initial_bearing(start_lat, start_lng, end_lat, end_lng):\n",
    "    lat1 = math.radians(start_lat)\n",
    "    lat2 = math.radians(end_lat)\n",
    "    diff_long = math.radians(end_lng - start_lng)\n",
    "\n",
    "    x = math.sin(diff_long) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - (\n",
    "        math.sin(lat1) * math.cos(lat2) * math.cos(diff_long)\n",
    "    )\n",
    "\n",
    "    initial_bearing = math.atan2(x, y)\n",
    "    initial_bearing = math.degrees(initial_bearing)\n",
    "    compass_bearing = (initial_bearing + 360) % 360\n",
    "    return compass_bearing\n",
    "\n",
    "def bearing_to_cardinal(bearing):\n",
    "    directions = [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]\n",
    "    ix = round(bearing / 45) % 8\n",
    "    return directions[ix]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_closest_osm_edge_old(lat, lng, edges_gdf,sindex=None,transformer=None):\n",
    "    # Validate: must already be in EPSG:3857\n",
    "    assert edges_gdf.crs.to_string().lower() == 'epsg:3857'\n",
    "\n",
    "    # Project the point manually\n",
    "    x, y = transformer.transform(lng, lat)\n",
    "    point = Point(x, y)\n",
    "\n",
    "    # Spatial index lookup\n",
    "    if sindex is None:\n",
    "        # Create spatial index if not provided\n",
    "        sindex = edges_gdf.sindex\n",
    "    candidates_idx = list(sindex.intersection(point.buffer(100).bounds))\n",
    "\n",
    "    if not candidates_idx:\n",
    "        return None  # or handle fallback\n",
    "\n",
    "    candidates = edges_gdf.iloc[candidates_idx]\n",
    "    distances = candidates.geometry.distance(point)\n",
    "    min_idx = distances.idxmin()\n",
    "    closest = candidates.loc[min_idx]\n",
    "\n",
    "    return {\n",
    "        'id': closest.get('id', None),\n",
    "        'geometry': closest.geometry,\n",
    "        'distance_meters': distances[min_idx]\n",
    "    }\n",
    "\n",
    "def find_closest_osm_edge(lat, lng, edges_gdf,edge_tree,transformer=None):\n",
    "    # Project GPS point\n",
    "    x, y = transformer.transform(lng, lat)\n",
    "    point = Point(x, y)\n",
    "\n",
    "    # STRtree nearest geometry\n",
    "    index = edge_tree.nearest(point)\n",
    "\n",
    "    # Look up original DataFrame index using WKB\n",
    "    #nearest_geom = edge_geometries.iloc[index]  # or .values[index] if using .values\n",
    "\n",
    "    # Lookup the original row from edges_gdf\n",
    "    edge_row = edges_gdf.iloc[index]\n",
    "\n",
    "    return {\n",
    "        'id': edge_row.get('id', None),\n",
    "        'geometry': edge_row.geometry,\n",
    "        'distance_meters': 0#point.distance(nearest_geom)\n",
    "    }\n",
    "\n",
    "def animate_vehicles(G, vehicle_paths, interval=10):\n",
    "    \"\"\"\n",
    "    Animate vehicles over a static OSMnx graph.\n",
    "\n",
    "    Parameters:\n",
    "    - G: OSMnx graph object\n",
    "    - vehicle_paths: List of dicts, each with {'id': ..., 'path': [(lon, lat), ...]}\n",
    "                     where path is ordered in 10s intervals\n",
    "    - interval: time step between positions in seconds\n",
    "    \"\"\"\n",
    "    # Step 1: Plot static graph (background)\n",
    "    fig, ax = ox.plot_graph(G,\n",
    "                            node_color='black', node_size=5,\n",
    "                            edge_linewidth=0.5, bgcolor='white',\n",
    "                            show=False, close=False, ax=None)\n",
    "\n",
    "    # Step 2: Initialize vehicle scatter points (1 per vehicle)\n",
    "    scatters = []\n",
    "    for vehicle in vehicle_paths:\n",
    "        lon, lat = vehicle['path'][0]\n",
    "        scatter = ax.scatter(lon, lat, c='red', s=20, label=f\"Vehicle {vehicle['id']}\")\n",
    "        scatters.append(scatter)\n",
    "\n",
    "    # Step 3: Animation update function\n",
    "    def update(frame):\n",
    "        for idx, vehicle in enumerate(vehicle_paths):\n",
    "            if frame < len(vehicle['path']):\n",
    "                lon, lat = vehicle['path'][frame]\n",
    "                scatters[idx].set_offsets([lon, lat])\n",
    "        return scatters\n",
    "\n",
    "    # Step 4: Run animation\n",
    "    total_frames = max(len(v['path']) for v in vehicle_paths)\n",
    "    ani = animation.FuncAnimation(fig, update, frames=total_frames,\n",
    "                                  interval=interval * 1000, blit=True, repeat=False)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_spatial_density_with_speed(df, dist_thresh=10, speed_diff_thresh=2):\n",
    "    \"\"\"\n",
    "    Compute density using distance AND speed similarity.\n",
    "\n",
    "    Inputs:\n",
    "    - df with columns: vehicle_id, time, lat, lon, speed, edge_id\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with time, edge_id, vehicle_count, \n",
    "        congested_pair_count, avg_speed, avg_pairwise_distance\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Project coordinates for accurate distance\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs='EPSG:4326')\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    grouped = gdf.groupby(['time', 'edge_id','cardinal'])\n",
    "\n",
    "    for (time, edge_id, cardinal), group in grouped:\n",
    "        if len(group) < 2:\n",
    "            results.append({\n",
    "                'time': time,\n",
    "                'edge_id': edge_id,\n",
    "                'cardinal': cardinal,\n",
    "                'vehicle_count': group['vehicle_id'].nunique(),\n",
    "                'congested_pair_count': 0,\n",
    "                'avg_speed': group['speed'].mean() if 'speed' in group else None,\n",
    "                'avg_pairwise_distance': None\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        coords = np.array([(geom.x, geom.y) for geom in group.geometry])\n",
    "        speeds = np.array(group['speed'])\n",
    "        vehicle_ids = np.array(group['vehicle_id'])\n",
    "\n",
    "        # Pairwise distances\n",
    "        dist_matrix = squareform(pdist(coords))\n",
    "        speed_matrix = squareform(pdist(speeds.reshape(-1, 1)))\n",
    "\n",
    "        # Create vehicle ID equality mask\n",
    "        same_vehicle_mask = np.equal.outer(vehicle_ids, vehicle_ids)\n",
    "\n",
    "        # Only consider pairs of different vehicles\n",
    "        valid_mask = (~same_vehicle_mask) & np.triu(np.ones_like(dist_matrix, dtype=bool), k=1)\n",
    "\n",
    "        # Identify congested pairs\n",
    "        congested_mask = (dist_matrix < dist_thresh) & (speed_matrix < speed_diff_thresh) & valid_mask\n",
    "        congested_pairs = np.sum(congested_mask)\n",
    "\n",
    "        # Average distance for valid (non-self) pairs\n",
    "        avg_dist = dist_matrix[valid_mask].mean() if valid_mask.any() else None\n",
    "        avg_speed = speeds.mean()\n",
    "\n",
    "        results.append({\n",
    "            'time': time,\n",
    "            'edge_id': edge_id,\n",
    "            'cardinal': cardinal,\n",
    "            'vehicle_count': group['vehicle_id'].nunique(),\n",
    "            'congested_pair_count': congested_pairs,\n",
    "            'avg_speed': avg_speed,\n",
    "            'avg_pairwise_distance': avg_dist\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def normalize_valhalla_route(route, route_index=0):\n",
    "    if \"summary\" not in route:\n",
    "        route=route.get(\"trip\", route)  # Fallback to trip if summary is missing\n",
    "    return {\n",
    "        \"index\": route_index,\n",
    "        \"summary\": route.get(\"summary\", {}),\n",
    "        \"leg\": route[\"legs\"][0] if route.get(\"legs\") else None,\n",
    "        \"distance_km\": route.get(\"summary\", {}).get(\"length\"),\n",
    "        \"duration_sec\": route.get(\"summary\", {}).get(\"time\"),\n",
    "    }\n",
    "\n",
    "async def async_get_routes_from_valhalla(session, origin, destination, max_nr_of_alternative_routes):\n",
    "    base_url = \"http://77.93.155.81:8002/route\"\n",
    "    payload = {\n",
    "        \"locations\": [\n",
    "            {\"lat\": origin[1], \"lon\": origin[0]},\n",
    "            {\"lat\": destination[1], \"lon\": destination[0]}\n",
    "        ],\n",
    "        \"costing\": \"auto\",\n",
    "    }\n",
    "\n",
    "    if max_nr_of_alternative_routes > 1:\n",
    "        payload['alternates'] = True\n",
    "        payload[\"number_of_alternates\"] = max_nr_of_alternative_routes - 1\n",
    "    else:\n",
    "        payload['alternates'] = False\n",
    "\n",
    "    async with session.post(base_url, json=payload) as response:\n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            raw_routes = []\n",
    "\n",
    "            trip = data.get(\"trip\")\n",
    "            if trip:\n",
    "                raw_routes.append(trip)\n",
    "\n",
    "            alternates = data.get(\"alternates\", [])\n",
    "            raw_routes.extend(alternates)\n",
    "\n",
    "            # Normalize all routes into a consistent format\n",
    "            return [normalize_valhalla_route(route, idx) for idx, route in enumerate(raw_routes)]\n",
    "        else:\n",
    "            print(f\"Error: {response.status} - {await response.text()}\")\n",
    "            return None\n",
    "\n",
    "def get_routes_from_valhalla(origin, destination,max_nr_of_alternative_routes):\n",
    "    base_url = \"http://77.93.155.81:8002/route\"\n",
    "    payload = {\n",
    "    \"locations\": [\n",
    "        {\"lat\": origin[1], \"lon\": origin[0]},\n",
    "        {\"lat\": destination[1], \"lon\": destination[0]}\n",
    "    ],\n",
    "    \"costing\": \"auto\",\n",
    "    }\n",
    "\n",
    "    if max_nr_of_alternative_routes > 1:\n",
    "        payload['alternates'] = True\n",
    "        payload[\"number_of_alternates\"] = max_nr_of_alternative_routes-1\n",
    "    else:\n",
    "        payload['alternates'] = False\n",
    "\n",
    "    \n",
    "    response = requests.post(base_url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        trip_data = response.json().get('trip', {})\n",
    "\n",
    "        # If 'routes' key exists, Valhalla gave main + alternates\n",
    "        routes_data = trip_data.get('routes')\n",
    "        if routes_data is not None:\n",
    "            return routes_data  # already a list of routes\n",
    "\n",
    "        # Else: only one route returned (no 'routes' key)\n",
    "        return [trip_data] \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    return None\n",
    "\n",
    "def convert_valhalla_leg_to_google_like_steps(leg):\n",
    "    full_coords = polyline.decode(leg['shape'], precision=6)  # full leg shape\n",
    "    maneuvers = leg['maneuvers']\n",
    "    \n",
    "    steps = []\n",
    "    for maneuver in maneuvers:\n",
    "        start_idx = maneuver['begin_shape_index']\n",
    "        end_idx = maneuver['end_shape_index']\n",
    "        coords = full_coords[start_idx:end_idx + 1]\n",
    "        step = {\n",
    "            'polyline': {\n",
    "                'points': coords  # OR use  for Google-style encoded string\n",
    "                \n",
    "            },\n",
    "            'duration': {\n",
    "                'value': maneuver['time']  # in seconds\n",
    "            },\n",
    "            'distance': {\n",
    "                'value': int(maneuver['length'] * 1000)  # convert km to meters\n",
    "            }\n",
    "        }\n",
    "        steps.append(step)\n",
    "    \n",
    "    return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6736feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Navigation():\n",
    "    def __init__(self, city_name, nr_vehicles=100, max_nr_of_alternative_routes=3, session=session, iteration_id = None, min_length = MIN_LENGTH, max_length = MAX_LENGTH, time_step = 10, time_window = 300,dist_thresh=10, speed_diff_thresh=2, slow_speed_thresh=5,alpha=1.0, beta=1.0, gamma=1.0,edges=None,nodes=None):\n",
    "        self.session = session\n",
    "        self.city_name = city_name\n",
    "        self.nr_vehicles=nr_vehicles\n",
    "        self.max_nr_of_alternative_routes=max_nr_of_alternative_routes\n",
    "        self.generate=True\n",
    "        self.dist_thresh=dist_thresh\n",
    "        self.speed_diff_thresh=speed_diff_thresh\n",
    "        print(datetime.now())\n",
    "\n",
    "\n",
    "        # 1. Check if city exists, if not , generate and store it in DB with nodes and edges\n",
    "        self.city = self.session.query(City).filter_by(name=city_name).first()\n",
    "        if not self.city:\n",
    "            print('City is not in the DB yet. Generating city...')\n",
    "            self.nodes,self.edges = self.get_city_graph()\n",
    "            self.store_city_to_db() \n",
    "            self.city = self.session.query(City).filter_by(name=city_name).first()   \n",
    "        else:\n",
    "            self.nodes, self.edges = self.get_city_data_from_db()\n",
    "        print(datetime.now())\n",
    "           \n",
    "\n",
    "        # 2. Check for existing identical run config\n",
    "        existing_run = session.query(RunConfig).filter_by(\n",
    "            city_id=self.city.id,\n",
    "            n_cars =self.nr_vehicles,\n",
    "            k_alternatives=self.max_nr_of_alternative_routes,\n",
    "            min_length = min_length,\n",
    "            max_length = max_length,\n",
    "            time_step = time_step,\n",
    "            time_window = time_window\n",
    "        ).first()\n",
    "\n",
    "        if existing_run:\n",
    "            self.runConfig=existing_run\n",
    "            print(f\" Run config already exists (run_id={existing_run.id}), skipping insertion.\")\n",
    "        else:\n",
    "            run_config = RunConfig(\n",
    "                city_id=self.city.id,\n",
    "                n_cars =self.nr_vehicles,\n",
    "                k_alternatives=self.max_nr_of_alternative_routes,\n",
    "                min_length = min_length,\n",
    "                max_length = max_length,\n",
    "                time_step = time_step,\n",
    "                time_window = time_window\n",
    "            )\n",
    "            self.session.add(run_config)\n",
    "            self.session.commit()\n",
    "            self.runConfig=run_config\n",
    "            print(f\" Run configuration saved (run_id={run_config.id}).\")\n",
    "        \n",
    "        \n",
    "        # Handle iteration creation\n",
    "        if iteration_id is not None:\n",
    "            existing_iteration = self.session.query(Iteration).filter_by(id=iteration_id, run_configs_id = self.runConfig.id).first()\n",
    "            if existing_iteration:\n",
    "                print(f\"Run configuration for iteration {iteration_id} already exists.\")\n",
    "                print(\"Stopping further execution due to condition.\")\n",
    "            else: \n",
    "                print(\"Please do not provide iteration_id, it will be generated accordingly.\")\n",
    "            return\n",
    "        else:\n",
    "            existing_iterations = self.session.query(Iteration).filter_by(run_configs_id=self.runConfig.id)\n",
    "            existing_iterations_count = existing_iterations.count()\n",
    "            self.iteration_id = existing_iterations_count + 1\n",
    "\n",
    "            iteration = Iteration(\n",
    "                iteration_id=self.iteration_id,\n",
    "                run_configs_id=self.runConfig.id\n",
    "            )\n",
    "            print(f\"Iteration created (iteration_id={self.iteration_id}) for run_config_id={self.runConfig.id}.\")\n",
    "        self.session.add(iteration)\n",
    "        self.session.commit()         \n",
    "\n",
    "                \n",
    "        print(datetime.now())        \n",
    "        if self.generate:\n",
    "            self.vehicles=self.generate_vehicles(iteration = self.iteration_id, min_length = min_length, max_length = max_length)\n",
    "            self.vehicles_routes=self.generate_vehicle_routes(API_KEY, iteration=self.iteration_id)\n",
    "        else:\n",
    "            print(datetime.now())\n",
    "\n",
    "            self.vehicles = self.session.query(Vehicle).filter_by(run_configs_id=self.runConfig.id, iteration_id=self.iteration_id).all()\n",
    "            print(datetime.now())\n",
    "\n",
    "            self.vehicles_routes = self.session.query(VehicleRoute).filter_by(run_configs_id=self.runConfig.id, iteration_id=self.iteration_id).all()\n",
    "\n",
    "        self.congestion_map=self.generate_congestion_map()\n",
    "        \n",
    "        self.plot_congestion_heatmap()\n",
    "       \n",
    "            \n",
    "    def get_city_graph(self):\n",
    "        # Get the graph for the city\n",
    "        G = ox.graph_from_place(self.city_name, network_type='drive')\n",
    "        \n",
    "        # Ensure the graph's CRS is WGS84 (EPSG:4326)\n",
    "        G.graph['crs'] = 'epsg:4326'\n",
    "        \n",
    "        # Convert the graph to GeoDataFrames for nodes and edges\n",
    "        nodes, edges = ox.graph_to_gdfs(G)\n",
    "        nodes = nodes.reset_index() #to have osmid as a column and not as an index\n",
    "        edges = edges.reset_index()\n",
    "        \n",
    "        # Now 'nodes' contains the intersection points (nodes) and 'edges' contains the road segments (edges)\n",
    "        \n",
    "        return nodes, edges\n",
    "    \n",
    "    def store_city_to_db(self):\n",
    "            node_count = len(self.nodes)\n",
    "            edge_count = len(self.edges)\n",
    "\n",
    "            city = City(\n",
    "                name=self.city_name,\n",
    "                node_count=node_count,\n",
    "                edge_count=edge_count,\n",
    "                created_at=datetime.utcnow()\n",
    "            )\n",
    "            self.session.add(city)\n",
    "            self.session.commit()  # Save city\n",
    "\n",
    "            # Prepare for DataFrame creation\n",
    "            node_records = []\n",
    "            edge_records = []\n",
    "\n",
    "            # Insert Nodes into the database\n",
    "            for _, node in self.nodes.iterrows():\n",
    "                node_data = {\n",
    "                    'city_id': city.id,\n",
    "                    'osmid': node.get('osmid', None),\n",
    "                    'x': node['x'] if not pd.isna(node['x']) else None,\n",
    "                    'y': node['y'] if not pd.isna(node['y']) else None,\n",
    "                    'street_count': node['street_count'] if not pd.isna(node.get('street_count', None)) else None,\n",
    "                    'highway': node['highway'] if not pd.isna(node.get('highway', None)) else None,\n",
    "                    'railway': node['railway'] if not pd.isna(node.get('railway', None)) else None,\n",
    "                    'junction': node['junction'] if not pd.isna(node.get('junction', None)) else None,\n",
    "                    'geometry': str(node['geometry']) if node['geometry'] is not None else None\n",
    "                }\n",
    "                node_records.append(node_data)\n",
    "                self.session.add(Node(**node_data))\n",
    "\n",
    "            self.session.commit()\n",
    "\n",
    "            # Insert Edges into the database\n",
    "            for _, edge in self.edges.iterrows():\n",
    "                edge_data = {\n",
    "                    'city_id': city.id,\n",
    "                    'u': edge.get('u', None),\n",
    "                    'v': edge.get('v', None),\n",
    "                    # 'osmid': edge['osmid'] if not pd.isna(edge.get('osmid', None)) else None,edges_gdf\n",
    "                    'length': str(edge['length']) if not pd.isna(edge.get('length', None)) else None,\n",
    "                    'geometry': str(edge['geometry']) if edge['geometry'] is not None else None\n",
    "                }\n",
    "                edge_records.append(edge_data)\n",
    "                self.session.add(Edge(**edge_data))\n",
    "\n",
    "            self.session.commit()\n",
    "            return city\n",
    "\n",
    "    def get_city_data_from_db(self):\n",
    "        # Fetch nodes and edges for a specific city\n",
    "\n",
    "        nodes_query = self.session.execute(\n",
    "                text(\"SELECT id, geometry AS geometry FROM nodes WHERE city_id = {}\".format(self.city.id))    \n",
    "        ).fetchall()\n",
    "        \n",
    "        edges_query = self.session.execute(\n",
    "                text(\"SELECT id, geometry AS geometry FROM edges WHERE city_id = {}\".format(self.city.id))  \n",
    "        ).fetchall()\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        print('nodes_query')\n",
    "        print(datetime.now())\n",
    "        nodes_df = pd.DataFrame(nodes_query, columns=[\"id\", \"geometry\"])\n",
    "        print('edges_query')\n",
    "        print(datetime.now())\n",
    "        edges_df = pd.DataFrame(edges_query, columns=[\"id\", \"geometry\"])\n",
    "        # Convert the WKT geometry to Shapely geometries\n",
    "        nodes_df['geometry'] = nodes_df['geometry'].apply(wkt.loads)\n",
    "        edges_df['geometry'] = edges_df['geometry'].apply(wkt.loads)\n",
    "        print('nodes_df')\n",
    "        # Create GeoDataFrame from DataFrame\n",
    "        nodes_gdf = gpd.GeoDataFrame(nodes_df, geometry='geometry', crs='EPSG:4326')\n",
    "        edges_gdf = gpd.GeoDataFrame(edges_df, geometry='geometry', crs='EPSG:4326')\n",
    "        \n",
    "        return nodes_gdf, edges_gdf\n",
    "    \n",
    "\n",
    "        \n",
    "    def plot_congestion_heatmap(self, cmap='Reds', figsize=(12, 12)):\n",
    "        \"\"\"\n",
    "        Aggregates congestion data over time and plots a heatmap over the city's road network.\n",
    "        \"\"\"\n",
    "        # Ensure congestion_map is assigned\n",
    "        if not hasattr(self, 'congestion_map') or self.congestion_map.empty:\n",
    "            print(\"No congestion map data to plot.\")\n",
    "            return\n",
    "\n",
    "        # Merge and clean\n",
    "        merged_gdf = self.edges.merge(self.congestion_map, left_on='id', right_on='edge_id', how='left')\n",
    "        merged_gdf['congestion_score'] = merged_gdf['congestion_score'].fillna(0)\n",
    "\n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        merged_gdf.plot(\n",
    "            column='congestion_score',\n",
    "            cmap=cmap,\n",
    "            linewidth=2,\n",
    "            ax=ax,\n",
    "            legend=True,\n",
    "            legend_kwds={'label': \"Congestion Score\", 'shrink': 0.5}\n",
    "        )\n",
    "        ax.set_title('Traffic Congestion Heatmap')\n",
    "        ax.set_axis_off()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def generate_vehicles(self, iteration, min_length, max_length):\n",
    "        vehicles = []\n",
    "        vehicle_id = 0\n",
    "\n",
    "        \n",
    "        #### added for uniform source and destination selection KMeans \n",
    "        from sklearn.cluster import KMeans\n",
    "        def sample_spatially_diverse_edge(edges_gdf, n_clusters=500):\n",
    "            \"\"\"\n",
    "            Select a single edge from a spatial cluster to ensure geographic diversity.\n",
    "            \"\"\"\n",
    "            edges_proj = edges_gdf.to_crs(epsg=3857)  # Web Mercator projection in meters\n",
    "            centroids = edges_proj.geometry.centroid\n",
    "            coords = np.array([[p.x, p.y] for p in centroids])\n",
    "            kmeans = KMeans(n_clusters=min(n_clusters, len(edges_gdf)), n_init='auto').fit(coords)\n",
    "            edges_gdf['cluster'] = kmeans.labels_\n",
    "            sampled_cluster = random.choice(edges_gdf['cluster'].unique())\n",
    "            return edges_gdf[edges_gdf['cluster'] == sampled_cluster].sample(n=1).iloc[0]\n",
    "        ####\n",
    "\n",
    "        for _ in range(self.runConfig.n_cars):\n",
    "            valid_vehicle = False\n",
    "            retries = 0\n",
    "            max_retries = 100  # prevent infinite loops\n",
    "\n",
    "            while not valid_vehicle and retries < max_retries:\n",
    "                retries += 1\n",
    "\n",
    "                # Randomly select a source edge and point\n",
    "                source_edge = sample_spatially_diverse_edge(self.edges)#self.edges.sample(n=1).iloc[0]\n",
    "                source_position_on_edge = random.random()\n",
    "                source_line = source_edge['geometry']\n",
    "                source_point = source_line.interpolate(source_position_on_edge, normalized=True)\n",
    "\n",
    "                # Randomly select a destination edge and point\n",
    "                destination_edge = sample_spatially_diverse_edge(self.edges)#self.edges.sample(n=1).iloc[0]\n",
    "                destination_position_on_edge = random.random()\n",
    "                destination_line = destination_edge['geometry']\n",
    "                destination_point = destination_line.interpolate(destination_position_on_edge, normalized=True)\n",
    "\n",
    "                # Calculate distance\n",
    "                distance = geodesic((source_point.y, source_point.x), (destination_point.y, destination_point.x)).meters\n",
    "\n",
    "                if min_length <= distance <= max_length:\n",
    "                    valid_vehicle = True\n",
    "                    vehicle_id += 1\n",
    "                    vehicle = {\n",
    "                        'vehicle_id': vehicle_id,\n",
    "                        'source_edge_id': source_edge['id'],\n",
    "                        'source_position_on_edge': source_position_on_edge,\n",
    "                        'source_geometry': source_point,\n",
    "                        'destination_edge_id': destination_edge['id'],\n",
    "                        'destination_position_on_edge': destination_position_on_edge,\n",
    "                        'destination_geometry': destination_point\n",
    "                    }\n",
    "\n",
    "                    vehicledb = Vehicle(\n",
    "                        vehicle_id=vehicle_id,\n",
    "                        run_configs_id=self.runConfig.id,\n",
    "                        iteration_id=iteration,\n",
    "                        source_edge_id=vehicle['source_edge_id'],\n",
    "                        source_position_on_edge=vehicle['source_position_on_edge'],\n",
    "                        source_geometry=vehicle['source_geometry'],\n",
    "                        destination_edge_id=vehicle['destination_edge_id'],\n",
    "                        destination_position_on_edge=vehicle['destination_position_on_edge'],\n",
    "                        destination_geometry=vehicle['destination_geometry']\n",
    "                    )\n",
    "                    vehicles.append(vehicle)\n",
    "                    self.session.add(vehicledb)\n",
    "            if not valid_vehicle:\n",
    "                print(f\"⚠️  Skipped a vehicle after {max_retries} retries due to distance constraints.\")\n",
    "\n",
    "        self.session.commit()\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "        # Convert to GeoDataFrame\n",
    "        vehicles_df = pd.DataFrame(vehicles)\n",
    "        #vehicles_df.head(10)\n",
    "        vehicles_df['source_geometry'] = vehicles_df['source_geometry'].apply(lambda x: Point(x.x, x.y))\n",
    "        vehicles_df['destination_geometry'] = vehicles_df['destination_geometry'].apply(lambda x: Point(x.x, x.y))\n",
    "        vehicles_gdf = gpd.GeoDataFrame(vehicles_df)\n",
    "\n",
    "        return vehicles_gdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ...existing code...\n",
    "    def generate_vehicle_routes(self, api_key, iteration):\n",
    "        def get_points_in_time_window(steps, time_step=self.runConfig.time_step, time_window=self.runConfig.time_window): \n",
    "            iter_step=0\n",
    "            time_start=0\n",
    "            points=[] \n",
    "            for time_step in range(time_step,time_window,time_step):\n",
    "                for index in range(len(steps))[iter_step:]:\n",
    "                    if index>=iter_step:\n",
    "                        step=steps[index]\n",
    "                        if time_start<=time_step and time_start+step['duration']['value']>time_step:\n",
    "                            #print(time_step)\n",
    "                            polyline_points = step['polyline']['points']\n",
    "                            gdf_line = create_linestring_from_polyline(polyline_points)\n",
    "                            point_on_line = get_point_on_line(gdf_line.geometry[0], (time_step-time_start)/step['duration']['value'])  # Get the midpoint\n",
    "                            points.append({'location':point_on_line,'time':time_step,'speed':step['distance']['value']/step['duration']['value'] if step['duration']['value']!=0 else 0})\n",
    "                            break\n",
    "                        if time_step+time_step>time_start+step['duration']['value']:\n",
    "                            time_start+=step['duration']['value']\n",
    "                            iter_step=index+1\n",
    "                    \n",
    "            return points\n",
    "        \n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()  # Needed if running in Jupyter\n",
    "\n",
    "        print('Starting async route fetching...')\n",
    "        print(datetime.now())\n",
    "        async def fetch_all_routes(vehicles, max_nr_of_alternative_routes):\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                tasks = []\n",
    "                for _, vehicle in vehicles.iterrows():\n",
    "                    origin = (vehicle['source_geometry'].x, vehicle['source_geometry'].y)\n",
    "                    destination = (vehicle['destination_geometry'].x, vehicle['destination_geometry'].y)\n",
    "                    tasks.append(async_get_routes_from_valhalla(session, origin, destination, max_nr_of_alternative_routes))\n",
    "                return await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Run async fetching\n",
    "        loop = asyncio.get_event_loop()\n",
    "        routes_data_list = loop.run_until_complete(fetch_all_routes(self.vehicles, self.max_nr_of_alternative_routes))\n",
    "        print(datetime.now())\n",
    "        routes = []\n",
    "        edges_gdf = self.edges.to_crs(epsg=3857)\n",
    "        #sindex=edges_gdf.sindex\n",
    "\n",
    "        # --- Step 2: Build a pure list of geometries (not a GeoSeries) ---\n",
    "        #edge_geometries = list(edges_gdf['geometry'])  # This is key!\n",
    "        edge_geometries = [geom if isinstance(geom, BaseGeometry) else geom.__geo_interface__ for geom in edges_gdf['geometry'].values.tolist()]\n",
    "\n",
    "        # --- Step 3: Build STRtree from that list ---\n",
    "        edge_tree = STRtree(edge_geometries)\n",
    "\n",
    "        # --- Step 4: Create a geometry → DataFrame index map using WKB ---\n",
    "        #geom_index_map = {geom.wkb: idx for geom, idx in zip(edge_geometries, edges_gdf.index)}\n",
    "\n",
    "        # --- Step 5: Transformer for input lat/lng to projected CRS ---\n",
    "        transformer = Transformer.from_crs(\"EPSG:4326\", edges_gdf.crs, always_xy=True)\n",
    "\n",
    "\n",
    "        for vehicle_idx, vehicle in self.vehicles.iterrows():\n",
    "            routes_data = routes_data_list[vehicle_idx]\n",
    "            origin = (vehicle['source_geometry'].x, vehicle['source_geometry'].y)\n",
    "            if not routes_data:\n",
    "                print('error')\n",
    "                continue\n",
    "            \n",
    "            for route_id, route in enumerate(routes_data, start=1):\n",
    "                summary = route['summary']\n",
    "                vehicle_route = VehicleRoute(\n",
    "                    vehicle_id=vehicle['vehicle_id'],\n",
    "                    run_configs_id=self.runConfig.id,\n",
    "                    iteration_id=iteration,\n",
    "                    route_id=route_id,\n",
    "                    duration=summary['time'],\n",
    "                    distance=summary['length'] * 1000,\n",
    "                    duration_in_traffic=None\n",
    "                )\n",
    "                self.session.add(vehicle_route)\n",
    "                #self.session.commit()\n",
    "\n",
    "                point_id = 0\n",
    "                valhala_steps = convert_valhalla_leg_to_google_like_steps(route['leg'])\n",
    "                points = get_points_in_time_window(valhala_steps)\n",
    "                for point in points:\n",
    "                    previous_point = None\n",
    "                    if point_id > 0:\n",
    "                        previous_point = points[point_id - 1]\n",
    "                    else:\n",
    "                        previous_point = {'location': Point(origin[0], origin[1]), 'time': 0, 'speed': 0}\n",
    "\n",
    "                    point_id += 1\n",
    "                    lat = point['location'].y\n",
    "                    lon = point['location'].x\n",
    "                    time = point['time']\n",
    "                    speed = point['speed']\n",
    "                    #print(f\"Processing point {point_id} for vehicle {vehicle['vehicle_id']} at time {time} with speed {speed} m/s\")\n",
    "                    #print(datetime.now())\n",
    "                    edge = find_closest_osm_edge(lon, lat, edges_gdf, edge_tree,transformer=transformer)\n",
    "                    edge_id = edge['id']\n",
    "                    #print(f\"Closest edge ID: {edge_id}, Distance: {edge['distance_meters']} meters\")\n",
    "                    #print(datetime.now())\n",
    "                    bearing = calculate_initial_bearing(previous_point['location'].y, previous_point['location'].x, lat, lon)\n",
    "                    #print(bearing)\n",
    "                    #print(datetime.now())\n",
    "                    cardinal = bearing_to_cardinal(bearing)\n",
    "                    route_point = RoutePoint(\n",
    "                        vehicle_id=vehicle_route.vehicle_id,\n",
    "                        run_configs_id=vehicle_route.run_configs_id,\n",
    "                        iteration_id=vehicle_route.iteration_id,\n",
    "                        route_id=vehicle_route.route_id,\n",
    "                        point_id=point_id,\n",
    "                        edge_id=edge_id,\n",
    "                        cardinal=cardinal,\n",
    "                        speed=speed,\n",
    "                        lat=lat,\n",
    "                        lon=lon,\n",
    "                        time=time\n",
    "                    )\n",
    "                    self.session.add(route_point)\n",
    "                    routes.append(vehicle_route)\n",
    "        self.session.commit()\n",
    "        print(datetime.now())\n",
    "        return routes\n",
    "    # ...existing code...\n",
    "\n",
    "\n",
    "  \n",
    "    def generate_congestion_map(self):\n",
    "\n",
    "        # Delete old congestion map entries for this iteration\n",
    "        self.session.query(CongestionMap).filter_by(iteration_id=self.iteration_id).delete()\n",
    "        self.session.commit()\n",
    "\n",
    "        # Execute SQL query and fetch results\n",
    "        self.session.execute(sa.text(\"SET @dist_thresh := :dist_thresh\"), {'dist_thresh': self.dist_thresh})\n",
    "        self.session.execute(sa.text(\"SET @speed_diff_thresh := :speed_diff_thresh\"), {'speed_diff_thresh': self.speed_diff_thresh})\n",
    "        self.session.execute(sa.text(\"SET @iteration := :iteration_id\"), {'iteration_id': self.iteration_id})\n",
    "        self.session.execute(sa.text(\"SET @run_configs_id := :run_configs_id\"), {'run_configs_id': self.runConfig.id})\n",
    "\n",
    "        result = self.session.execute(sa.text(\"\"\"\n",
    "            SELECT\n",
    "                edge_id,\n",
    "                SUM(CASE \n",
    "                    WHEN distance < @dist_thresh AND speed_diff < @speed_diff_thresh THEN \n",
    "                        (1 / ((1 + distance) * (1 + speed_diff)))\n",
    "                    ELSE 0\n",
    "                END) AS weighted_congestion_score\n",
    "            FROM (\n",
    "                SELECT\n",
    "                    a.edge_id,\n",
    "                    6371 * 2 * ASIN(SQRT(\n",
    "                        POW(SIN(RADIANS(b.lat - a.lat) / 2), 2) +\n",
    "                        COS(RADIANS(a.lat)) * COS(RADIANS(b.lat)) *\n",
    "                        POW(SIN(RADIANS(b.lon - a.lon) / 2), 2)\n",
    "                    )) AS distance,\n",
    "                    ABS(a.speed - b.speed) AS speed_diff\n",
    "                FROM trafficOptimization.route_points a\n",
    "                JOIN trafficOptimization.route_points b\n",
    "                    ON a.time = b.time\n",
    "                    AND a.edge_id = b.edge_id\n",
    "                    AND a.cardinal = b.cardinal\n",
    "                    AND a.vehicle_id < b.vehicle_id\n",
    "                WHERE a.iteration_id = @iteration\n",
    "                AND b.iteration_id = @iteration\n",
    "                AND a.run_configs_id = @run_configs_id\n",
    "                AND b.run_configs_id = @run_configs_id\n",
    "            ) AS pairwise\n",
    "            GROUP BY edge_id;\n",
    "        \"\"\"), {\n",
    "            'dist_thresh': self.dist_thresh,\n",
    "            'speed_diff_thresh': self.speed_diff_thresh,\n",
    "            'iteration_id': self.iteration_id,\n",
    "            'run_configs_id': self.runConfig.id\n",
    "        })\n",
    "\n",
    "        congestion_data = result.fetchall()\n",
    "\n",
    "        for row in congestion_data:\n",
    "            congestion_map = CongestionMap(\n",
    "                run_configs_id=self.runConfig.id,\n",
    "                iteration_id=self.iteration_id,\n",
    "                edge_id=row.edge_id,\n",
    "                congestion_score=row.weighted_congestion_score\n",
    "            )\n",
    "            self.session.add(congestion_map)\n",
    "\n",
    "        self.session.commit()\n",
    "\n",
    "        # Return results as DataFrame\n",
    "        df_result = pd.DataFrame(congestion_data, columns=['edge_id', 'congestion_score'])\n",
    "        return df_result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8daea76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-02 13:09:53.712840\n",
      "nodes_query\n",
      "2025-06-02 13:09:55.888299\n",
      "edges_query\n",
      "2025-06-02 13:09:55.889820\n",
      "nodes_df\n",
      "2025-06-02 13:09:55.949843\n",
      " Run config already exists (run_id=2), skipping insertion.\n",
      "Iteration created (iteration_id=3) for run_config_id=2.\n",
      "2025-06-02 13:09:56.646624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m case1=\u001b[43mNavigation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCITY_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnr_vehicles\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mNavigation.__init__\u001b[39m\u001b[34m(self, city_name, nr_vehicles, max_nr_of_alternative_routes, session, iteration_id, min_length, max_length, time_step, time_window, dist_thresh, speed_diff_thresh, slow_speed_thresh, alpha, beta, gamma, edges, nodes)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(datetime.now())        \n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28mself\u001b[39m.vehicles=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miteration_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m.vehicles_routes=\u001b[38;5;28mself\u001b[39m.generate_vehicle_routes(API_KEY, iteration=\u001b[38;5;28mself\u001b[39m.iteration_id)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 255\u001b[39m, in \u001b[36mNavigation.generate_vehicles\u001b[39m\u001b[34m(self, iteration, min_length, max_length)\u001b[39m\n\u001b[32m    252\u001b[39m source_point = source_line.interpolate(source_position_on_edge, normalized=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Randomly select a destination edge and point\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m destination_edge = \u001b[43msample_spatially_diverse_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#self.edges.sample(n=1).iloc[0]\u001b[39;00m\n\u001b[32m    256\u001b[39m destination_position_on_edge = random.random()\n\u001b[32m    257\u001b[39m destination_line = destination_edge[\u001b[33m'\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 233\u001b[39m, in \u001b[36mNavigation.generate_vehicles.<locals>.sample_spatially_diverse_edge\u001b[39m\u001b[34m(edges_gdf, n_clusters)\u001b[39m\n\u001b[32m    231\u001b[39m edges_proj = edges_gdf.to_crs(epsg=\u001b[32m3857\u001b[39m)  \u001b[38;5;66;03m# Web Mercator projection in meters\u001b[39;00m\n\u001b[32m    232\u001b[39m centroids = edges_proj.geometry.centroid\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m coords = np.array([[\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m, p.y] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m centroids])\n\u001b[32m    234\u001b[39m kmeans = KMeans(n_clusters=\u001b[38;5;28mmin\u001b[39m(n_clusters, \u001b[38;5;28mlen\u001b[39m(edges_gdf)), n_init=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m).fit(coords)\n\u001b[32m    235\u001b[39m edges_gdf[\u001b[33m'\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m'\u001b[39m] = kmeans.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rr642bg\\OneDrive - Technicka univerzita v Kosiciach\\Desktop\\qa_mtc\\venv\\Lib\\site-packages\\shapely\\geometry\\point.py:91\u001b[39m, in \u001b[36mPoint.x\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mx\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     90\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return x coordinate.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mshapely\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_x\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rr642bg\\OneDrive - Technicka univerzita v Kosiciach\\Desktop\\qa_mtc\\venv\\Lib\\site-packages\\shapely\\decorators.py:87\u001b[39m, in \u001b[36mmultithreading_enabled.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[32m     86\u001b[39m         arr.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rr642bg\\OneDrive - Technicka univerzita v Kosiciach\\Desktop\\qa_mtc\\venv\\Lib\\site-packages\\shapely\\_geometry.py:298\u001b[39m, in \u001b[36mget_x\u001b[39m\u001b[34m(point, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;129m@multithreading_enabled\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_x\u001b[39m(point, **kwargs):\n\u001b[32m    275\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the x-coordinate of a point.\u001b[39;00m\n\u001b[32m    276\u001b[39m \n\u001b[32m    277\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m \n\u001b[32m    297\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "case1=Navigation(CITY_NAME,nr_vehicles=5000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7c294",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "\n",
    "0. handle error\n",
    "```\n",
    "Starting async route fetching...\n",
    "2025-06-02 12:35:30.200064\n",
    "Error: 400 - {\"error_code\":442,\"error\":\"No path could be found for input\",\"status_code\":400,\"status\":\"Bad Request\"}\n",
    "Error: 400 - {\"error_code\":442,\"error\":\"No path could be found for input\",\"status_code\":400,\"status\":\"Bad Request\"}\n",
    "Error: 400 - {\"error_code\":442,\"error\":\"No path could be found for input\",\"status_code\":400,\"status\":\"Bad Request\"}\n",
    "2025-06-02 12:36:43.213135\n",
    "```\n",
    "\n",
    "++ preco mi to generuje vsetky auta na jednej kope - vyriesila som to KMeans, ale pomale to je\n",
    "\n",
    "1. select only 0.9 percentile of the vehicles that cause congestion\n",
    "```\n",
    "def filter_routes_for_qubo(routes_df, congestion_df, threshold=0.9):\n",
    "    high_congestion_edges = congestion_df[congestion_df['congestion_score'] > congestion_df['congestion_score'].quantile(threshold)]\n",
    "    congested_edge_ids = set(high_congestion_edges['edge_id'])\n",
    "\n",
    "    filtered_vehicles = []\n",
    "    for vehicle_id, group in routes_df.groupby('vehicle_id'):\n",
    "        used_edges = set(group['edge_id'])\n",
    "        if used_edges & congested_edge_ids:\n",
    "            filtered_vehicles.append(vehicle_id)\n",
    "    return filtered_vehicles\n",
    "```\n",
    "\n",
    "2. construct QUBO only for these filtered_vehicles\n",
    "\n",
    "3. calculate weights per vehicle pair and routes. w[i,i,k] =0 and w[i,j,k] = weighted_congestion_score from sql/weights.sql\n",
    "\n",
    "4. calculate normalized weights and set lambda  = 1,2,3,... or somehow calculate lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82efdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a51556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
